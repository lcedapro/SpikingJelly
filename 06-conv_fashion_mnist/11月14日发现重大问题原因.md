今天发现PB和SJ输出不一致的重大问题的原因，先说结论

## 结论：

输入从0-1转为0-255之后不能简单的将阈值vthr乘以255，因为输入数据是先乘weight再加bias，所以在缩放的时候应该先把weight乘以255bias不动vthr也不动，要么就是weight不动bias乘255vthr才能乘255

之前相当于是weight和bias都没动，只有vthr乘了255，所以第0层网络输出的完全是错误的结果

## 问题发现的过程：

11月14日完成了[PB和SJ对比可视化代码](paibox_spikingjelly_compare_vis_alexnet.py)的编写，用原来的conv2int权重文件将4.npy每层输出的特征图可视化之后发现一模一样，跟11月8号[simpletest](paibox_spikingjelly_compare_simpletest.py)的运行结果一样（就是PAIBox比SpikingJelly每层晚一个钟，但是错开比较的特征图是完全一样的）。

一开始以为是[PB和SJ对比可视化代码](paibox_spikingjelly_compare_vis_alexnet.py)中的`image = (image * 255).astype(np.uint8)`向下取整造成的精度偏差。但是改成四舍五入`image = (image * 255).round().astype(np.uint8)`跑出来的结果还是一模一样。

把[PB和SJ对比主程序](paibox_spikingjelly_compare_main_alexnet.py)的SpikingJelly网络输入也改成了0-255（同时第0层阈值改为*255），发现Spikingjelly的结果跟PAIBox一模一样，准确率都是50%多，说明都是错误的。

后来就想到，输入图像放大255倍应该只和第0层的weight有关，和bias和vthr无关，所以正确的做法应该是修改conv2int的代码，把第0层的weight缩小255倍再转int。（见本文档后面的代码）

但是这样子搞，第0层的权重太小了，权重最大值0.009343153*118只有1.1，导致第0层乘118之后取整之后都是1和0，量化误差疑似有点过于大了，导致[infer_bn2conv_alexnet.py](infer_bn2conv_alexnet.py)跑出来的acc都是0.1，跟猜的一样。

所以，根本就不能既用一个0-255的灰度图片的数据集又用卷积层直接编码，因为这样子的话，第0层的权重就太小了。只能用输入比较小（最好只有0和1，就像每层SNN的输出那样）的数据集（比如N-MNIST或者DVS Gesture），才能跑直接编码。

## 总结：

这几天南辕北辙了，又是换网络，又是可视化，就是没发现这个小问题，南辕北辙了呜呜呜

以后06-conv_fashion_mnist文件夹就不在维护了，全面转向神经形态数据集（N-MNIST或者DVS Gesture）的实验了。

## 附：

11月14日修改后的model_parameters_conv2int_alexnet.py如下：
```python
# 下述代码实现将原权重文件的所有参数缩放至-128到127后取整，并打印出新网络各层LIF的Vthr值。原权重文件的参数以float32格式存储，新权重文件的参数以int32格式存储。

import torch
import numpy as np

def multply_model(model, multiplier):
    for name, param in model.items():
        if isinstance(param, torch.Tensor):
            # model[name] = (param * multiplier).to(torch.int8)
            model[name] = (param * multiplier).to(torch.int8)
    return model

def manual_multply_model(model, layer_name_list: list, mult_list: list):
    for layer_name, mult in zip(layer_name_list, mult_list):
        if layer_name + '.weight' in model:
            model[layer_name + '.weight'] = (model[layer_name + '.weight'] * mult).to(torch.int8)
            print("Layer " + layer_name + " weight multiplied by " + str(mult) + " and converted to int32.")
        else:
            print("Layer " + layer_name + " does not exist or has no weight.")
        if layer_name + '.bias' in model:
            model[layer_name + '.bias'] = (model[layer_name + '.bias'] * mult).to(torch.int8)
            print("Layer " + layer_name + " bias multiplied by " + str(mult) + " and converted to int32.")
        else:
            print("Layer " + layer_name + " does not exist or has no bias.")
    return model

def maxium_multply_model(model, layer_name_list: list):
    """
    每层参数分别缩放并取整数，缩放系数由int(127.0/该层参数最大值)，该缩放系数作为新网络各层LIF的Vthr参数
    但是，对于第0层网络，由于转为整数域后，输出的图像表示方式也从[0, 1]变为[0, 255]，因此第0层网络的weight要先自己除以255，再缩放并取整数（该功能被弃用，改为在主函数中实现）
    """
    multiplied_model = model.copy()
    for layer_name in layer_name_list:
        max_weight = 0
        max_bias = 0
        if layer_name + '.weight' in model:
            weight_array = model[layer_name + '.weight'].detach().cpu().numpy()
            max_weight = np.max(np.abs(weight_array))
            print("Layer " + layer_name + " max weight is " + str(max_weight))
        if layer_name + '.bias' in model:
            bias_array = model[layer_name + '.bias'].detach().cpu().numpy()
            max_bias = np.max(np.abs(bias_array))
            print("Layer " + layer_name + " max bias is " + str(max_bias))
        max_value = max(max_weight, max_bias)
        print("Layer " + layer_name + " max value is " + str(max_value))
        mult = int(127.0 / float(max_value))
        print("Layer " + layer_name + " multiply factor is " + str(mult))
        if layer_name + '.weight' in model:
            multiplied_model[layer_name + '.weight'] = (model[layer_name + '.weight'] * mult).to(torch.int8)
        if layer_name + '.bias' in model:
            multiplied_model[layer_name + '.bias'] = (model[layer_name + '.bias'] * mult).to(torch.int8)
    return multiplied_model

def main():
    # 加载原始权重文件
    checkpoint_path = './logs/T4_b1024_sgd_lr0.1_c16_amp_cupy_alex_dec1/checkpoint_max_bn2conv.pth'
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # 将所有参数乘以MULT后取整
    multiplied_checkpoint = checkpoint.copy()
    # 对于第0层网络，由于转为整数域后，输出的图像表示方式也从[0, 1]变为[0, 255]，因此第0层网络的weight要先自己除以255，再缩放并取整数
    multiplied_checkpoint['net']['conv_fc.0.weight'] = (multiplied_checkpoint['net']['conv_fc.0.weight'] / 255.0).to(torch.float32)
    layer_name_list = ['conv_fc.0', 'conv_fc.2', 'conv_fc.4', 'conv_fc.6', 'conv_fc.8', 'conv_fc.11', 'conv_fc.13', 'conv_fc.15']
    multiplied_checkpoint['net'] = maxium_multply_model(multiplied_checkpoint['net'], layer_name_list)

    # 保存新的权重文件
    multiplied_checkpoint_path = './logs/T4_b1024_sgd_lr0.1_c16_amp_cupy_alex_dec1/checkpoint_max_conv2int.pth'
    torch.save(multiplied_checkpoint, multiplied_checkpoint_path)
    print("所有参数乘并转int8完成，新权重文件保存为 " + multiplied_checkpoint_path)

if __name__ == "__main__":
    main()

# 运行结果：
# Layer conv_fc.0 max weight is 0.009343153
# Layer conv_fc.0 max bias is 1.0698292
# Layer conv_fc.0 max value is 1.0698292
# Layer conv_fc.0 multiply factor is 118
# Layer conv_fc.2 max weight is 0.2496002
# Layer conv_fc.2 max bias is 1.7862588
# Layer conv_fc.2 max value is 1.7862588
# Layer conv_fc.2 multiply factor is 71
# Layer conv_fc.4 max weight is 0.3296965
# Layer conv_fc.4 max bias is 1.3522851
# Layer conv_fc.4 max value is 1.3522851
# Layer conv_fc.4 multiply factor is 93
# Layer conv_fc.6 max weight is 0.2619039
# Layer conv_fc.6 max bias is 0.8791547
# Layer conv_fc.6 max value is 0.8791547
# Layer conv_fc.6 multiply factor is 144
# Layer conv_fc.8 max weight is 0.30725917
# Layer conv_fc.8 max bias is 1.593527
# Layer conv_fc.8 max value is 1.593527
# Layer conv_fc.8 multiply factor is 79
# Layer conv_fc.11 max weight is 0.051310923
# Layer conv_fc.11 max value is 0.051310923
# Layer conv_fc.11 multiply factor is 2475
# Layer conv_fc.13 max weight is 0.09354615
# Layer conv_fc.13 max value is 0.09354615
# Layer conv_fc.13 multiply factor is 1357
# Layer conv_fc.15 max weight is 0.32684073
# Layer conv_fc.15 max value is 0.32684073
# Layer conv_fc.15 multiply factor is 388
```